---
title:  "Probability and Statistics"
excerpt: "Deep Learning Summary"

tags:
  - [deep learning, machine learning, AI, School Summary]

toc: true
toc_sticky: true
 
date: 2022-09-20
last_modified_at: 2022-09-20
---

# 2.1 확률과 통계

# 2.1.1 확률 기초

### 확률 변수

- 나올 수 있는 경우의 수 집합 중 하나를 갖는 변수
- 정의역은 나올 수 있는 모든 경우를 원소로 갖는다

### 확률 벡터

- 확률 변수 여러개를 순차적으로 배열하여 표시한것
  > $
  x=(x_1,x_2,x_3,x_4)^T\\
  x1,x2,x3,x4는\ 각각\ 확률\ 변수
  $

### 확률 함수

- 입력은 확률 변수이고 함수값은 확률(0~1 사이)을 가지는 함수
- 주로 $P(x=확률 변수/벡터)$ 로 나타냄

### 확률 분포

- 사건이 발생 할 수 있는 확률을 나타낸 것
- 확률 질량/밀도 함수등으로 나타낸다
![확률 분포 그래프](/assets/img/statisticsGraph.png)

### 조건부 확률

- 조건이 있는 확률
- 특정 사건 $B$가 발생한 후 다른 특정사건 A가 발생할 확률
- 표기법
  > $
  P(A|B)=\frac{P(AB)}{P(A)}
  $

곱 규칙

- 여러 사건이 동시에 발생할 확률을 조건부 확률로 나타내는 방법
  > $P(AB)=P(B|A)P(A)$

합 규칙
- 하나의 사건이 일어날 확률을 다른 사건들과 조건부 확률로 나타내는 방법
  > $P(A)=\displaystyle\sum_{B}{P(BA)} = \displaystyle\sum_{B}{P(A|B)P(B)}$ 

# 2.2.2 베이즈 정리와 기계학습

### 베이즈 정리

- 베이지안 주의
  - 특정사건이 발생했다는 가설의 신뢰도를 의미
- 베이즈 정리
  - $P(H)$<sup>$\leftarrow 사전확률$</sup>과 P(H|E)<sup>$\leftarrow 사후확률$</sup>사이의 관계에 대한 정리

  - 사건 E<sup>Evidence</sup> 발생 전후로 사건 H<sup>Hypothesis</sup>가 발생하였다는 가설의 신뢰도를 계산하는법
  - 사건 E 이전의 H의 신뢰도 $P(H)$는 사전확률, 발생 이후 H의 신뢰도 $P(H|E)$는 사후확률이다.
    > $ P(HE)$ $=P(E|H)P(H)=P(EH)=P(H|E)P(E)\\\rightarrow P(H|E)=\frac{P(E|H)P(H)}{P(E)}$

- 기계학습과 베이즈정리
  - E라는 입력이 주어졌을때 사후확률 $P(H|E)$ 확률을 베이즈 정리를 통해 추정
  - 기계학습에서의 용어 해설
    - $H$ : 데이터를 통해 추정하고자 하는 값
      - linear regression에서는 함수의 weight, classification 문제의 경우 이산적인 변수
    - $E$ : 특징벡터/훈련데이터
    - $P(H)$ = H일 확률 $\rightarrow$ 사전확률
    - $P(E)$ = 데이터 E의 분포
      - 특징공간에 반비례, 이 확률은 같기 때문에 생략가능
    - $P(H|E)$ = 특징벡터 E가 주어졌을 때 H의 분포, 확률 $\rightarrow$ 사후확률
      - 특정 훈련 데이터가 특정 값 H일 확률
    - $P(E|H)$ = H가 특정 분류, 값일 때 특징벡터가 E일 확률 $\rightarrow$ 우도<sup>가능도</sup>

# 2.2.3 최대 우도

### 우도란?

- 확률의 반대 의미
- H가 발생했을때 E가 H 이전에 발생했었을 가능성 $\\ \rightarrow$ 즉 목적함수의 결과값이 특정 특징벡터로 부터 파생되었을 확률

### 최대우도법<sup>Maximum Likelihood Estimation</sup>

- 사후확률 즉 훈련데이터 $E$에 맞는 $H$를 찾는 방법 
- $P(H|E)=\frac{P(E|H)P(H)}{P(E)}$에서 $P(E), P(H)$가 동일하다는 가정을 하면 $P(E|H)$<sup>우도</sup>가 사후확률과 비례하므로 우도가 가장 높게 되는 H을 추정하는 방식이다

# 2.2.4 평균과 분산

### 데이터 요약 정보로서 평균과 분산

- 평균
  > $\mu=\frac{1}{n}\displaystyle\sum_{i=1}^{n}{x_i}$
- 분산
  > $\sigma^2=\frac{1}{n}\displaystyle\sum_{i=1}^{n}{(x_i-\mu)^2}$

### 벡터의 평균과 분산

- 평균 벡터
  > $\mu=\frac{1}{n}\displaystyle\sum_{i=1}^{n}{x_i}$
- 공분산 행렬
  > $\Sigma=\frac{1}{n}\displaystyle\sum_{i=1}^{n}{(x_i-\mu)(x_i-\mu)^T}$


# 2.2.5 유용한 확률분포

- 가우시안 분포
  - 평균 $\mu$과 분산 $\sigma^2$로 정의
    > $N(x;\mu,\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2) $

    ![가우시안분포](/assets/img/%EA%B0%80%EC%9A%B0%EC%8B%9C%EC%95%88%EB%B6%84%ED%8F%AC.png)
  - 다차원 가우시안 분포
    - 평균벡터 $\mu$와 공분산 행렬 $\Sigma$로 정의
      > $N(x;\mu,\Sigma)=\frac{1}{\sqrt{|\Sigma|}\sqrt{(2\pi)^d}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$

- 베르누이 분포
  - 베르누이 실행에 따른 분포
  - 베르누이 실행
    - 결과가 두 가지 중 하나로만 나오는 실험이나 시행
      > $Ber(x;p)=\mu^x(1-\mu)^{1-x} = \begin{cases}\mu&if\ 시행이\ 성공할경우,\ x=1\\1-\mu&if\ 시행이\ 실패할경우,\ x=0\end{cases}$

- 이항분포
  - 성공확률이 $\mu$인 베르누이 실험을 m번 실행할 때 성공할 횟수 $x$에 대한 확률 분포
    > $B(x;m,\mu) = C^{x}_m\mu^x(1-\mu)^{m-x}=\frac{m!}{x!(m-x)!}\mu^x(1-\mu)^{m-x}$
    
      ![이항분포 사진]()

# 2.2.6 정보이론

- 메시지가 지닌 정보를 수량화 하는 이론
- 기본 원리
  - 확률이 작을수록 많은 정보를 가진다

### 자기 정보

- 사건 $e_i$의 정보량

  > $log_2P(e_i)$

### 엔트로피

- 확률 변수 x의 불확실성을 나타내는 값

### 교차 엔트로피

- 두 확률분포 사이의 관계를 나타낸다.

### KL 다이버전스

- 두 확률분포 사이의 거리