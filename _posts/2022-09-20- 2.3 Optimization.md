---
title:  "Optimization"
excerpt: "Deep Learning Summary"

categories:
  - A.I
  
tags:
  - [deep learning, machine learning, AI, School Summary]

toc: true
toc_sticky: true
layout: post
use_math: true
 
date: 2022-09-20
last_modified_at: 2022-09-20
---

# 2.3 최적화

### **2.3.1 매개변수 공간의 특징**

- 신경망에서 각 노드가 입력값에 곱하는 가중치 공간
- 노드, 레이어의 갯수에 비례하기 때문에 특징공간보다 더 높은 차원을 갖는다.

### **2.3.2 매개변수 공간의 탐색**

- 차원의 저주<sup>1.2.3절</sup> 때문에 참인 확률분포를 구하는 것이 불가능해서 기계학습은 다음과 같은 전력을 가진다.
  
  1. 적절한 모델 설정
  2. 목적함수설정(가중치 벡터 설정)
  3. 목적함수의 매개변수 공간에서 최적점 탐색<sup>가중치 벡터 최적화</sup>



### **최적화<sup>Optimization</sup> 문제**

- 목적함수 $J(\Theta)$의 최적점 혹은 최적점과 비슷한 지역 최적점인 $\hat{\Theta}$ 을 구하는 문제
  > $\hat{\Theta}=argminJ(\Theta)$
  
    ![매개변수 공간에 최적해](/assets/img//%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%EA%B3%B5%EA%B0%84%ED%83%90%EC%83%89.png)

- 완전 탐색<sup>낱낱탐색 알고리즘</sup>
  - 가능한 모든 매개변수 공간을 전부 탐색
  - 차원이 너무 높으면 시간이 기하급수적으로 오래 걸린다

- 랜덤 탐색
  - 매개변수 공간에서 무작위 해를 랜덤하게 계속해서 고르는 방식
  - 아무런 전략이 없음

- 경사하강법
  - 매개변수 공간에서 랜덤한 한 점을 잡고 미분값을 통해 목적함수의 값이 낮아지는 방향으로 탐색하는것

### 2.3.2 미분

- 미분에 의한 최적화
  - 1차 도함수 $f'(x)$ = 함수의 기울기, 즉 값이 커지는 방향
  - 즉 목적함수의 도함수는 값이 작아지는 방향(최적점 방향)의 반대방향에 최적점

- 편미분
  - 변수가 여러개인 함수의 미분
  - 각각의 변수로 미분한 값들이 이루는 벡터를 그레디언트<sup>gradiant</sup>라고 한다
    > $\nabla f = \frac{\partial f}{\partial 𝐱} = (\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2})^T$
  - 최적화 목표인 매개변수 벡터 $\Theta$에 변수가 많기 때문에 편미분 사용

- 연쇄법칙
  -합성함수의 미분법
    > $f(x)=g(h(x)) \\ \rightarrow f'(x) = g'(h(x))h'(x) \\ f(x)= g(h(i(x))) \\ \rightarrow f'(x)=g'(h(i(x)))h'(i(x))i'(x)$
  - 다층 퍼셉트론은 함성함수이기 때문에 연쇄법칙 사용

- 야코비안 행렬
- 해시안 행렬

### 2.3.3 경사하강법

- 미분을 통해 매개변수 공간에서 목적함수의 값이 낮은곳으로 가는 방법
   > $𝐠 = \Delta \Theta = \frac{\partial J}{\partial \Theta} \\ \rho =  학습률 \\ \rightarrow \Theta=\Theta-\rho 𝐠$



- 배치경사하강법
  - 훈련 집합 전체의 그래디언트를 계산 후 평균값을 이용하여 $\Theta$를 한번에 갱신

- 스토캐스틱 경사하강법
  - 훈련 집합에서 랜덤한 부분집합을 만들어 그래디언트를 계산 후 평균값을 이용하여 $\Theta$를 갱신