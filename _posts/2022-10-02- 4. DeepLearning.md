---
title:  "Deep Learning"
excerpt: "Deep Learning Summary"


categories:
  - A.I
  
tags:
  - [deep learning, machine learning, AI, School Summary]

toc: true
toc_sticky: true
layout: post
use_math: true
 
date: 2022-10-02
last_modified_at: 2022-10-02
---

# **4. 딥러닝**

### **4.1. 딥러닝이란?**

- <span style="color:orange">깊은 신경망</span>을 <span style="color:gold">학습</span>시키는 알고리즘

### **4.1.1. 깊은 신경망<sup>Deep Multi-Layer Perceptron</sup>**

- 다층 퍼셉트론에 <span style="color:orange">은닉층</span>이 여러개 추가된 <span style="color:gold">신경망</span>
- 기존 다층퍼셉트론<sup>MLP</sup>과 차이는 거의 없음
  - 그나마 활성함수, 목적함수의 변화
  - 활성함수로 <span style="color:red">ReLu</span>, 목적함수로 <span style="color:red">교차엔트로피 목적함수</span>를 주로 사용한다.

### **4.1.2. 딥러닝의 문제점과 해결 방안**

| 종류 | 원인 |해결 방법|
|:---:|:---|:---|
|그레디언트 소멸 문제|작은 가중치를 곱하다보니 값이 점점 작아져서 소멸|잔류학습<sup>ResNet</sup> <br> 활성함수 개선, ReLU 활성함수 등장<br>층별 사전학습<sup>pretraining</sup>|
|훈련 집합의 크기|훈련을 시킬 수 있는 데이터가 적음|인터넷 발전으로 학습데이터의 양과 질 향상|
|과다한 계산 시간|학습을 위해선 값비싼 슈퍼컴퓨터가 필요|값싼 GPU의 등장|
|과잉적합<sup>overfitting</sup>|신경망이 학습데이터에 너무 적합한 나머지 일반화 능력이 하락|다양한 규제기법|
|그 외|-|컴볼루션 신경망을 통해 딥러닝의 가능성을 높임|

### **4.1.3. 기존 기계학습과 차이**

|기계학습|딥러닝|
|:---|:---|
|센서나 관찰로 통해 얻은 raw 데이터를 사람이 가공하여 특징을 추출하는 과정이 필요|입력데이터로 <span style="color:orange">raw 데이터를 학습</span> 가능</br>은닉층에서 <span style="color:red">특징학습</span>을 통해 특징을 자체적으로 추출하도록 학습</br>자연스럽게 앞의 레이어는 저급특징, 뒤의 레이어는 고급특징을 추출|
|![머신러닝](/assets/img/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5%EA%B0%9C%EC%9A%94.png)|![딥러닝](/assets/img/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EC%9A%94.png)|

### **4.2. CNN**<sup>Convolutional Neural Network</sup>

- Convolution, 풀링 연산등을 통해 이루어진 신경망

- **특징**
  - 완전연결구조인 DMLP와 다르게 <span style="color:gold">부분 연결 구조</span>
    - 이전 레이어 노드중 일부만 연결
  - 격자구조(영상, 음성)의 데이터에 적합
  - <span style="color:orange">가변크기의 데이터</span> 처리 가능
  - 좋은 특징을 추출하게 된다
    - 인접한 픽셀들만 이용하기 때문에 상관도가 떨어지는 멀리 떨어진 픽셀값은 무시됨
    - 즉 이전 레이어에서 상관도가 높은 노드들만 사용하여 가중합 계산

### **4.2.1. Convolution 연산**

- 수용장 부분과 커널에서 해당하는 요소끼리 곱하여 특징맵을 추출하는 연산

- **특징맵**
  - Convolution 연산을 통해 추출된 <span style="color:orange">특징을 담은 맵</span>
  - 벡터, 행렬, 텐서의 형태

- **커널**
  - <span style="color:orange">특징을 추출</span>하기 위한 일종의 <span style="color:red">필터</span>
  - 사람이 설정하는게 아니라 특징학습을 통해 알아내게 되는 값
  - 특정 두 은닉층 사이에서는 같은 커널을 사용하여 <span style="color:gold">가중치 공유</span>를 한다
    - 모델의 복잡도가 낮아짐
  - 하나의 커널은 한가지의 특징만 추출하기 때문에 신경망에서 하나의 레이어에 대해 여러개의 커널을 사용하여 <span style="color:gold">다중 특징 맵</span>을 추출

- **수용장**
  - 연산에 사용할 노드의 범위
  - 수용장을 이동시키며 특징맵을 추출
  - 커널과 사이즈가 같음


  ![머신러닝](/assets/img/%ED%95%A9%EC%84%B1%EA%B3%B1%EC%97%B0%EC%82%B0.png)

### **4.2.2. 풀링 연산**

- 특징맵, 이전노드의 <span style="color:gold">결과를 모으는 연산</span>
- 학습 할 커널이 없고, <span style="color:skyblue">하이퍼 파라미터(개발자가 정하는 값)</span>만 존재
- 최대풀링, 평균풀링, 가중치 평균(가중치를 곱한값들의 평균) 풀링등이 존재
- 풀링사이즈
  - 몇개의 노드를 모을지 정한 파라미터
  - 수용장과 비슷한 개념
- 특징
  - 상세내용에서 요약 통계를 추출
  - 작은 이동에 둔감
    - 물체, 영상 검색등에 효과적

- **4.2.3. CNN 연산 기법**
  - 덧대기
    - 모서리부분에서는 컨볼루션을 할 수 없기 때문에 feature의 크기가 줄어든다
    - 전처리로 끝에 데이터를 추가하여 연산 전후로 특징맵의 크기를 유지시키는 기법

    ![머신러닝](/assets/img/%EB%8D%A7%EB%8C%80%EA%B8%B0.png)

  - 바이어스 추가
    - 연산을 할 때 연산 결과에 bias를 추가하여 특징맵을 생성하는 기법

    ![머신러닝](/assets/img/%EB%B0%94%EC%9D%B4%EC%96%B4%EC%8A%A4%EC%97%B0%EC%82%B0.png)

  - 컨볼루션 연산에 따른 cnn의 특성
    - 이동에 동변

  - 보폭
    - 풀링이나 컴볼루션 연산을 할때 <span style="color:orange">수용장을 얼마나 뛰어넘어서 할지 정하는 값</span>
    - 크면 클수록 특징맵이 작아지는 다운샘플링(피쳐맵 사이즈 감소) 발생
    - 해당 값을 통해 가변 크기의 특징맵을 다룰 수 있다.

    ![보폭](/assets/img/%EB%B3%B4%ED%8F%AD.png)

### **4.2.4. CNN 표현법**

- 빌딩 블록
  - 특징 맵, 입력데이터, 커널을 빌딩모양의 블록으로 나타내는 방법
  - 빌딩 사이의 연산은 블록 사이의 화살표로 표시

  ![머신러닝](/assets/img/%EB%B9%8C%EB%94%A9%EB%B8%94%EB%A1%9D.png)


### **4.3. CNN 사례연구**

- LeNet-5
  - s=보폭
  - h=깊이
  - k=커널 개수
  - p=패딩

- vggnet
  - 3x3의 작은 커널을 사용하여 신경망을 더욱 깊게 만듬
  - 파라미터수를 줄이고 활성함수를 많이 만들어 비선형성을 잘 나타낼 수 있다
  - 1x1커널
    - 차원축소 효과 존재

- GoogLeNet
  - NIN구조
    - MLPconv가 컨볼루션 연산을 대신함
    - 커널을 적용할 때 커널 범위 내의 특징맵을 완전 연결하여 가중치를 곱한다.
    - 알번 convolution 연산은 해당하는 위치의 노드에만 가중치를 곱해서 더한다
  -전역 평균풀링
    - 몰?루

- ResNet
  - 잔류학습
    - 층을 늘리는 과정에서 그래디언트가 소멸되는 문제 해결
    - 특정 노드의 출력을 다음 레이어 노드 뿐 아니라 더 뒤의 레이어 노드의 입력에 더한다.(지름길 생성)

### 4.4. 딥러닝이 강력한 이유

- 통째학습
- 은닉층이 길다
  - 퍼셉트론이 20개일때 노드가 20개짜리 은닉층 하나보다 노드 10개짜리 은닉층 2개가 더 정교한 분할이 가능하다.
  - 층이 많기 때문에 층의 역할 구분이 뚜렷하게 나타날 수 있다.