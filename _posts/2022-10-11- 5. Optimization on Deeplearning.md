---
title:  "Optimization on Deeplearning"
excerpt: "Deep Learning Summary"

# categories:
#   - Blog

tags:
  - [deep learning, machine learning, AI, School Summary]

toc: true
toc_sticky: true
layout: post
use_math: true
 
date: 2022-10-11
last_modified_at: 2022-10-11
---

# **5. 최적화**

- 목적함수를 정의하고 최적해를 구하는 것

- 목적함수의 비볼록성질<sup>해가 1개가 아님</sup>, 고차원 특징공간, 데이터의 희소성 등으로 기계학습에서 최적화는 어려운 문제

# **5.1. 목적함수**

### **목적함수란?**

- <span style="color:skyblue">훈련 데이터와 신경망</span> 결과사이의  <span style="color:gold">오차를 정량화</span> 해서 나타내어 주는 함수

### **평균제곱 오차/MSE<sup>Minimum Square Error</sup> 목적함수**

- 보통 <span style="color:skyblue">활성함수 $\sigma$</span>로 $\frac{e^x}{e^x+1}$<span style="color:orange"><sup>$=$ 시그모이드 로지스틱 함수</sup></span>를 사용
- Perceptron의 결과값 $o$와 훈련 집합에 따른 기대값 $y$의 차의 제곱들의 평균
- $2$차 $Norm$으로 표현
  > $e=\frac{1}{2} \left\| y-o \right\|_2^2\\ = \frac{1}{2}(y-o)^2 \\ = \frac{1}{2}(y-\sigma(\displaystyle\sum_{i=1}^{d}{W_ix_i}+b))^2$
  - 온라인 모드에서는 미분 후 최고차항의 계수를 1로 유지하기 위해 평균이 아닌 2로 나눔
- 오차가 클수록 목적함수의 값이 커지기 때문에 <span style="color:red">벌점<sup>panalty</sup></span>으로 사용

- **MSE 목적함수의 단점**
  - 벌점의 크기에 비례하여 경사하강을 할 때, 벌점의 크기와 실제 gradiant의 값이 아래 사진과 같이 정비례 하지 않는 경우가 존재해 효율적으로 학습이 이루어지지 않음
  ![](/assets/img/MSE%EB%8B%A8%EC%A0%90.png)
  - **단점의 원인**
    - 위 그림에 퍼셉트론의 gradiant를 계산하면 아래 식이 나온다. 이때 gradiant에서 $\sigma'(wx+b)$의 값이 아래 그래프와 같이 $wx+b$가 0일 때를 기준으로 감소하기 때문에 gradiant와 벌점이 비례하지 않는다.
      > $e=\frac{1}{2}(y-\sigma(wx+b))^2\ 일때 \\ \frac{\partial e}{\partial o} = (\frac{\partial e}{\partial w},\frac{\partial e}{\partial b})^T \\ \frac{\partial e}{\partial w}=-x\sigma'(wx+b)(y-\sigma(wx+b)) \\\frac{\partial e}{\partial b}= -\sigma'(wx+b)(y-\sigma(wx+b))$

      ![시그모이드 함수](/assets/img/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98.png)

### **교차 엔트로피 목적함수**

- 보통 <span style="color:skyblue">활성함수 $\sigma$</span>로 $\frac{e^x}{e^x+1}$<span style="color:orange"><sup>$=$ 시그모이드 로지스틱 함수</sup></span>를 사용
- Perceptron의 결과값 $o$와 훈련 집합에 따른 기대값 $y$의 차의 제곱들의 평균

### **로그 우도 목적함수와 Softmax 활성함수**

- **로그 우도 목적함수**
  - 다른 목적함수들과 다르게 모든 출력 노드값의 가중합을 사용하지 않고 <span style="color:orange">하나의 노드에서 오는 입력</span>만 사용
    > $e=-\log_2o_y$
  - 보통 Softmax함수를 활성함수로 사용
  - <span style="color:red">***이전레이어와 노드개수가 같음?***</span>

- **Softmax 활성함수**
  - Softmax는 최댓값이 아닌 값을 억제하여 0에 가깝게, 최대값은 1에 가깝게 만드는 활성함수
  - 해당 레이어의 결과 값들의 합은 1이 되어 확률을 모방.
    > $o_j=\frac{e^{s_j}=현재\ 노드의\ 목적함수\ 결과}{\displaystyle\sum_{i=1}^{c}{e^{s_i}}= 해당 레이어의\ 모든\ 노드\ 목적함수\ 결과의\ 합}$

    ![소프트맥스, 시그모이드 비교](/assets/img/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%EB%B9%84%EA%B5%90.png)
- Softmax 활성함수는 최대값이 아닌 값을 0으로 억제하고, 하나의 노드만 사용하는 로그 우도 목적함수는 훈련집합의 결과에 해당하는 부류만 선택하여 볼 수 있어 자주 <span style="color:gold">Softmax활성함수와 로그우도 목적함수를 결합하여 사용</span>하는 경우가 많음

# **5.2. 딥러닝 학습 기법/요령**

### **데이터 전처리**

- **규모문제**
  - 다른 특징에 비해 데이터 사이의 편차가 많이 나지 않는 특징과 관련된 가중치는 학습속도가 드림
    - 1.885m와 1.525m는 33cm나 차이가 나지만 특징값 차이는 불과 0.33 
    - 65.5kg과 45.0kg은 20.5라는 차이
    - 첫번째와 두번째 특징은 대략 100배의 규모 차이

- **모든 특징이 양수인 문제**
  - 최저점을 찾아가는 경로가 오락가락해서 수렴에 걸리는데 늦어짐
  -<span style="color:orange">

- **정규화<sup>Normalize</sup>**
  - 각 특징의 <span style="color:orange">평균과 편차</span>로 아래식과 같이 정규화
  - 평균과 표준편차가 1이 되기 때문에 규모문제와 특징이 양수인 문제가 해결
     > $x_i^{new} = \frac{x_i^{old}-\mu^{평균}}{\sigma^{편차}}$

    ![정규화](/assets/img/normalize.png)

- **one-hot코드**
  - 데이터 값에 거리 개념이 없는 특징<sup>명칭값</sup>은 원핫 코드로 변경하여 각 특징의 분류마다 비트를 부여
  - 해당 값이 아니면 아에 틀린 값이기 때문에 분리하여 예측
    ![정규화](/assets/img/onehot.png)

### **가중치 초기화**

- 대칭적 가중치 문제
  - 아래 사진의 왼쪽과 같이 초기가중치가 대칭을 이루게 되면 각 gradiant도 똑같이 나와 두 노드가 하는 일에 중복성이 발생
  - 아래 사진의 오른쪽과 같이 초기 가중치를 랜덤값을 통해 초기화를 한다
    ![정규화](/assets/img/%EB%8C%80%EC%B9%AD%EC%A0%81%EA%B0%80%EC%A4%91%EC%B9%98.png)

  - 보통 가우시안, 균일분포등에서 난수를 추출하지만 성능차이는 없어 난수 분포도는 중요하지 않음
  - 반대로 난수 범위는 매우 중요
    - 아래 식을 통해 $[-r,r]$사이의 값을 난수 추출한다.
    - 난수의 범위가 크고 노드의 입출력 엣지가 많을수록 오버피팅이 발생할 확률이 높아 두 값을 반비례하도록 설정한다.
    ![정규화](/assets/img/%EB%82%9C%EC%88%98%EC%B6%94%EC%B6%9C.png)

  - 바이어스는 보통 0으로 초기화

### **모멘텀**

- 학습 방향을 유지하려는 관성
- gradiant는 미분값이기 때문에 노이즈가 발생할 확률이 높음
- gradiant에 아래 식과 그림과 같이 기존에 이동했던 그래디언트값의 누적분을 더하여 스무딩을 가함
- gradiant에 가속도를 주어 아래 사진과 같이 local minimum에 갇히는것을 해결
  >$v^{모멘텀}=\alpha v^{이전 모멘텀} - \rho\frac{\partial J}{\partial \Theta}^{현재 위치에서 gradiant} \\ \Theta=\Theta+v $

  ![모멘텀](/assets/img/%EB%AA%A8%EB%A9%98%ED%85%80.png)
  
- **$\alpha$의 효과**
  - 해당 값에 따라 이전 그레이디언트 정보를 얼마나 비중있게 할 것인지 결정
  - 1에 가까울수록 $\Theta$가 그리는 궤적이 매끄러워져서<sup>오버슈팅 효과 완화</sup> 0.5에서 0.99까지 학습을 하며 늘림

- **네스테로프 모멘텀**

  - 현재 모멘텀을 통해 이동할 $\tilde\Theta$를 계산후 해당 위치에서 gradiant $\frac{\partial J}{\partial\tilde\Theta}=\frac{\partial J}{\partial\Theta}|_{\tilde\Theta}$를 계산 후, $\tilde\Theta$와 $\frac{\partial J}{\partial\Theta}|_{\tilde\Theta}$ 를 이용하여 경사하강하는 방법
  ![네스테로프](/assets/img/%EB%84%A4%EC%8A%A4%ED%85%8C%EB%A1%9C%ED%94%84.png)

### **학습률<sup>$\rho$</sup>**
- 학습률이 너무 크면 오버슈팅에 따른 진자현상, 너무 작으면 학습에 오랜시간이 걸림
  > ![학습률](/assets/img/%ED%95%99%EC%8A%B5%EB%A5%A0.png)
- **적응형 학습률**
  - gradiant 각각의 요소 $\frac{\partial J}{\partial\Theta}=(\frac{\partial J}{\partial\theta_1},\frac{\partial J}{\partial\theta_2},...,\frac{\partial J}{\partial\theta_k})^T$에 각각 알맞은 크기의 학습률을 적용

  - **AdaGrad**<sup>adaptive gradiant</sup>
  - **RMSProp**
  - **Adam**

### **활성함수**

- 퍼셉트론에서 입력신호의 <span style="color:orange">활성값<sup>가중합을 의미</sup>을 출력 신호로 변환</span>하는 함수

- **시대별 활성함수**

  ![시대별 활성함수](/assets/img/%ED%99%9C%EC%84%B1%ED%95%A8%EC%88%98.png)
  - ReLU 이전의 활성함수는 활성값이 커지면 미분값 즉 gradiant가 0에 가까워져 학습이 매우 느려지는 <span style="color:red">포화문제</span> 발생

- **ReLU<sup>Rectified Linear Unit</sup>**
  - 포화문제를 해소한 활성함수
    >$y=ReLU(z) = max(0,z)$
  - **ReLU 변형**
      ![ReLU](/assets/img/ReLU.png)
      - **softplus**
        - 미분이 불가능한 ReLU함수 문제를 개선한 함수
        - 통계적으로 ReLU보다 안좋은 결과가 나옴
      - **leakyReLU<sup>$\alpha$를 0.01로 사용</sup>, PReLU<sup>$\alpha$를 학습으로 알아냄</sup>**
        - ReLU의 활성값이 음의 수인 경우도 고려한 함수
  
          >$leackyReLU(z)=\begin{cases}z,& z\geq0\\\alpha z,&z\lt0\end{cases}	$

# **규제<sup>Ragularization</sup>의 필요성과 원리**

- **학습 모델의 일반화 능력**
  - 

- **과잉적합**
  - 모델이

# **규제 기법**

### **규제의 종류**

- 명시적 규제
- 비명시적 규제

### **가중치 벌칙**

### **조기멈춤**

### **데이터 확대**

### **드롭 아웃**

### **앙상블 기법**